{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "for _ in range(100):\n",
    "    env.render()\n",
    "    env.step(env.action_space.sample()) # take a random action\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CartPoleEnv' object has no attribute 'monitor'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-71e1b3f3b898>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    114\u001b[0m                        alpha=0.5, gamma=0.90, epsilon=0.1)\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cartpole-exp-1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi_episode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gym/gym/core.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"attempted to get missing private attribute '{}'\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'CartPoleEnv' object has no attribute 'monitor'"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Based the following two code bases:\n",
    " - Berkeley's CS188 pacman project code\n",
    "   http://ai.berkeley.edu/\n",
    " - Victor Mayoral Vilches's RL tutorial \n",
    "   https://github.com/vmayoral/basic_reinforcement_learning\n",
    "\n",
    "@author: Heechul Yun (heechul.yun@gmail.com)\n",
    "'''\n",
    "\n",
    "import gym\n",
    "import random,math\n",
    "import numpy\n",
    "import pandas\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "class QLearningAgent:\n",
    "    def __init__(self, actions, epsilon=0.1, gamma=0.90, alpha=0.5, **args):\n",
    "        self.alpha = alpha # learning rate\n",
    "        self.gamma = gamma # discount\n",
    "        self.epsilon = epsilon # exploration probability\n",
    "        self.actions = actions\n",
    "        self.qs = {} # state table\n",
    "\n",
    "    def getQValue(self, state, action):\n",
    "        if not (state in self.qs) or not (action in self.qs[state]):\n",
    "            return 0.0\n",
    "        else:\n",
    "            return self.qs[state][action]\n",
    "\n",
    "    def getLegalActions(self,state):\n",
    "        return self.actions\n",
    "\n",
    "    # def getAction(self, state):\n",
    "    #     action = None\n",
    "    #     if util.flipCoin(self.epsilon):\n",
    "    #         legalActions = self.getLegalActions(state)\n",
    "    #         action = random.choice(legalActions)\n",
    "    #     else:\n",
    "    #         action = self.computeActionFromQValues(state)\n",
    "    #     return action\n",
    "\n",
    "    def getAction(self, state):\n",
    "        \"\"\"\n",
    "          Compute the best action to take in a state.  Note that if there\n",
    "          are no legal actions, which is the case at the terminal state,\n",
    "          you should return None.\n",
    "        \"\"\"\n",
    "        legalActions = self.getLegalActions(state)\n",
    "        if len(legalActions) == 0:\n",
    "            return None\n",
    "        q = [self.getQValue(state, a) for a in legalActions]\n",
    "        maxQ = max(q)\n",
    "\n",
    "        # this is the trick.\n",
    "        if random.random() < self.epsilon:\n",
    "            minQ = min(q)\n",
    "            mag = max(abs(minQ), abs(maxQ))\n",
    "            q = [q[i] + random.random() * mag - 0.5 *mag for i in range(len(legalActions))]\n",
    "            maxQ = max(q)\n",
    "\n",
    "        count = q.count(maxQ)\n",
    "        if count > 1:\n",
    "            best = [i for i in range(len(legalActions)) if q[i] == maxQ]\n",
    "            i = random.choice(best)\n",
    "        else:\n",
    "            i = q.index(maxQ)\n",
    "        return legalActions[i]\n",
    "\n",
    "    def update(self, state, action, nextState, reward):\n",
    "        \"\"\"\n",
    "        Update q-value of the given state\n",
    "        \"\"\"\n",
    "        if not (state in self.qs):\n",
    "            self.qs[state] = {}\n",
    "        if not (action in self.qs[state]):\n",
    "            self.qs[state][action] = reward\n",
    "        else:\n",
    "            maxqnew = max([self.getQValue(nextState, a) for a in self.getLegalActions(nextState)])\n",
    "            diff = reward + self.gamma * maxqnew - self.qs[state][action]\n",
    "            newQ = self.qs[state][action] + self.alpha * diff\n",
    "            self.qs[state][action] = newQ\n",
    "\n",
    "        # print \"(s, a, s', r) = [%3d (%3.1f, %3.1f), %d, %3d (%3.1f, %3.1f), %.1f]\" % \\\n",
    "        #     (state, self.getQValue(state,0), self.getQValue(state, 1), action, \\\n",
    "        #      nextState, self.getQValue(nextState,0), self.getQValue(nextState, 1), \\\n",
    "        #      reward)\n",
    "\n",
    "\n",
    "def build_state(features):\n",
    "    return int(\"\".join(map(lambda feature: str(int(feature)), features)))\n",
    "\n",
    "def to_bin(value, bins):\n",
    "    return numpy.digitize(x=[value], bins=bins)[0]\n",
    "\n",
    "\n",
    "last100Scores = [0] * 100\n",
    "last100ScoresIndex = 0\n",
    "last100Filled = False\n",
    "\n",
    "# Number of states is huge so in order to simplify the situation\n",
    "# we discretize the space to: 10 ** number_of_features\n",
    "n_bins = 8\n",
    "n_bins_angle = 10\n",
    "cart_position_bins = pandas.cut([-2.4, 2.4], bins=n_bins, retbins=True)[1][1:-1]\n",
    "cart_velocity_bins = pandas.cut([-1, 1], bins=n_bins, retbins=True)[1][1:-1]\n",
    "pole_angle_bins = pandas.cut([-2, 2], bins=n_bins_angle, retbins=True)[1][1:-1]\n",
    "angle_rate_bins = pandas.cut([-3.5, 3.5], bins=n_bins_angle, retbins=True)[1][1:-1]\n",
    "\n",
    "last_time_steps = numpy.ndarray(0)\n",
    "\n",
    "agent = QLearningAgent(actions=range(env.action_space.n), \n",
    "                       alpha=0.5, gamma=0.90, epsilon=0.1)\n",
    "\n",
    "env.monitor.start('cartpole-exp-1', force=True)\n",
    "\n",
    "for i_episode in range(1000):\n",
    "    state = env.reset()\n",
    "\n",
    "    # if i_episode > 100:\n",
    "    #     agent.epsilon = 0.01\n",
    "\n",
    "    for t in range(200):\n",
    "        # env.render()\n",
    "\n",
    "        # choose an action\n",
    "        stateId = build_state([to_bin(state[0], cart_position_bins), \n",
    "                               to_bin(state[1], cart_velocity_bins),\n",
    "                               to_bin(state[2], pole_angle_bins),\n",
    "                               to_bin(state[3], angle_rate_bins)])\n",
    "        action = agent.getAction(stateId)\n",
    "\n",
    "        # perform the action\n",
    "        state, reward, done, info = env.step(action)\n",
    "        nextStateId = build_state([to_bin(state[0], cart_position_bins), \n",
    "                               to_bin(state[1], cart_velocity_bins),\n",
    "                               to_bin(state[2], pole_angle_bins),\n",
    "                               to_bin(state[3], angle_rate_bins)])\n",
    "\n",
    "        if done == False:\n",
    "            # update q-learning agent\n",
    "            agent.update(stateId, action, nextStateId, reward)\n",
    "        else:\n",
    "            reward = -200.0\n",
    "            agent.update(stateId, action, nextStateId, reward)\n",
    "            last100Scores[last100ScoresIndex] = t\n",
    "            last100ScoresIndex += 1\n",
    "            if last100ScoresIndex >= 100:\n",
    "                last100Filled = True\n",
    "                last100ScoresIndex = 0\n",
    "            if not last100Filled:\n",
    "                print(\"Episode \",i_episode,\" finished after {} timesteps\".format(t+1))\n",
    "            else:\n",
    "                print(\"Episode \",i_episode,\" finished after {} timesteps\".format(t+1),\" last 100 average: \",(sum(last100Scores)/len(last100Scores)))\n",
    "            last_time_steps = numpy.append(last_time_steps, [int(t + 1)])\n",
    "            break\n",
    "\n",
    "l = last_time_steps.tolist()\n",
    "l.sort()\n",
    "print(\"Overall score: {:0.2f}\".format(last_time_steps.mean()))\n",
    "print(\"Best 100 score: {:0.2f}\".format(reduce(lambda x, y: x + y, l[-100:]) / len(l[-100:])))\n",
    "\n",
    "env.monitor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
